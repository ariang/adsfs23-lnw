{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import overpy\n",
    "import psycopg2\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set option to see all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#open csv file from extracted immo entries\n",
    "immos = []\n",
    "with open('immo.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        print(' '.join(row).split(','))\n",
    "        immos.append(' '.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe and insert all the results\n",
    "df = pd.DataFrame()\n",
    "#do some string transformations and split the string into separate values\n",
    "for i in immos:\n",
    "    i = i.replace('\"','').replace('\\'','')\n",
    "    i = re.split(r'[()]', i)\n",
    "    i = [x for x in i if not(x=='' or x==',')]\n",
    "    df1 = pd.DataFrame(i)\n",
    "    df1 = df1[0].str.split(',', expand=True)\n",
    "    df = pd.concat([df,df1])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial dataframe length without filters\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the data which has m^2 information\n",
    "df = df[df[1].str.contains('m²')==True]\n",
    "df = df.reset_index(inplace=False, drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(pd.notnull(df), None)\n",
    "\n",
    "for i, rows in df.iterrows():\n",
    "    #print(print(rows[4]))\n",
    "    if(rows[4] is not None):\n",
    "        if(re.match(\"[A-Z]{2}$\",rows[4].lstrip())):\n",
    "            #print(i)\n",
    "            df[5][i] = df[4][i].lstrip()\n",
    "        if(re.search(r\"^\\d{4}\",rows[3].lstrip())):\n",
    "            #print(i)\n",
    "            df[4][i] = df[3][i].lstrip()\n",
    "            df[3][i] = 'nicht vorhanden'\n",
    "        if(re.search(r\"\\.\\—\",df[3][i])):\n",
    "                if(re.search(r\"^\\d{4}\",rows[4].lstrip())):\n",
    "                    df[2][i] = df[2][i]+(df[3][i].lstrip())\n",
    "                    df[3][i] = 'nicht vorhanden'\n",
    "                    #print(i)\n",
    "                elif(re.search(r\"^\\d{4}\",rows[5].lstrip())):\n",
    "                    df[2][i] = df[2][i]+(df[3][i].lstrip())\n",
    "                    df[3][i] = df[4][i].lstrip()\n",
    "                    df[4][i] = df[5][i].lstrip()\n",
    "                    df[5][i] = df[6][i].lstrip()\n",
    "                    df[6][i] = None\n",
    "                    #df[3][i] = 'nicht vorhanden'\n",
    "\n",
    "        if (rows[6] is not None):\n",
    "            if(re.match(\"[A-Z]{2}$\",rows[6].lstrip())):\n",
    "                #print(i)\n",
    "                df[4][i] = df[5][i].lstrip()\n",
    "                df[5][i] = df[6][i].lstrip()\n",
    "                df[6][i] = None\n",
    "            elif(re.match(r\"^\\d{4}\",rows[6].lstrip())):\n",
    "                df[4][i] = df[6][i].lstrip()\n",
    "                df[5][i] = df[7][i].lstrip()\n",
    "                df[6][i] = None\n",
    "                df[7][i] = None\n",
    "                #print(df.iloc[i])\n",
    "            elif(re.match(r\"^\\d{4}\",rows[7].lstrip())):\n",
    "                #df[5][i] = df[6][i].lstrip()\n",
    "                df[4][i] = df[7][i].lstrip()\n",
    "                df[5][i] = df[8][i].lstrip()\n",
    "                df[6][i] = None\n",
    "                df[7][i] = None\n",
    "                df[8][i] = None\n",
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if every row has a zip code\n",
    "import math\n",
    "for i, rows in df.iterrows():\n",
    "    if(rows[4] is  None):\n",
    "        print(i)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual data extension with the help of google\n",
    "df.loc[1269][4] = '4052 Basel'\n",
    "df.loc[1269][5] = 'BS'\n",
    "df.loc[1829][4] = '4051 Basel'\n",
    "df.loc[1829][5] = 'BS'\n",
    "df = df.drop([1880])\n",
    "df.loc[2261][4] = '1005 Lausanne'\n",
    "df.loc[2281][4] = '1018 Lausanne'\n",
    "df.loc[2281][5] = 'VD'\n",
    "df.loc[2385][4] = '3013 Bern'\n",
    "df.loc[2385][5] = 'BE'\n",
    "df.loc[2515][4] = '3007 Bern'\n",
    "df.loc[2515][5] = 'BE'\n",
    "df.loc[3397][4] = '6992 Lugano'\n",
    "df.loc[3614][4] = '2503 Biel/Bienne'\n",
    "df.loc[3614][5] = 'BE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the data which has room information\n",
    "df = df[df[0].str.contains('room')==True]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the data which has correct price information\n",
    "df = df[df[2].str.contains('CHF')==True]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split rooms into number and word\n",
    "df[0] = df[0].str.split()\n",
    "df[1] = df[1].str.split()\n",
    "df[2] = df[2].str.split()\n",
    "df[4] = df[4].str.split()\n",
    "df[6] = df[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only save number of rooms, m2 value and francs and split zip code into two columns\n",
    "for i,rows in df.iterrows():\n",
    "    rows[0] = float(rows[0][0])\n",
    "    rows[1] = int(rows[1][0])\n",
    "    rows[2] = int(rows[2][1].replace('.—',''))\n",
    "    rows[5] = str(rows[4][1])\n",
    "    rows[4] = int(rows[4][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create id\n",
    "df.insert(0, 'ID', range(1, len(df)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB API using Overpass Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the overpass query\n",
    "query = f'''\n",
    "[out:json];\n",
    "area[name=\"Zurich\"] -> .searchArea0;\n",
    "area[name=\"Geneva\"] -> .searchArea1;\n",
    "area[name=\"Basel\"] -> .searchArea2;\n",
    "area[name=\"Lausanne\"] -> .searchArea3;\n",
    "area[name=\"Bern\"] -> .searchArea4;\n",
    "area[name=\"Winterthur\"] -> .searchArea5;\n",
    "area[name=\"Luzern\"] -> .searchArea6;\n",
    "area[name=\"St. Gallen\"] -> .searchArea7;\n",
    "area[name=\"Lugano\"] -> .searchArea8;\n",
    "area[name=\"Biel\"] -> .searchArea9;\n",
    "( area.searchArea0; area.searchArea1; area.searchArea2; area.searchArea3; area.searchArea4; area.searchArea5; area.searchArea6; area.searchArea7; area.searchArea8; area.searchArea9;) -> .searchArea;\n",
    "(node[\"shop\"=\"supermarket\"](area.searchArea);way[\"shop\"=\"supermarket\"](area.searchArea););\n",
    "out center;\n",
    "'''\n",
    "\n",
    "# creating Overpass API object\n",
    "api = overpy.Overpass()\n",
    "\n",
    "# performing the query and retrieveing the results\n",
    "response = api.query(query)\n",
    "\n",
    "#create dataframe and append ways\n",
    "rdf= pd.DataFrame()\n",
    "for i in response.ways:\n",
    "    if(\"addr:postcode\" in i.tags and \"name\" in i.tags):\n",
    "            tdf = pd.DataFrame(i.tags,index=[i.id])\n",
    "            tdf[\"type\"] = 'way'\n",
    "            rdf = pd.concat([rdf,tdf])\n",
    "\n",
    "#append nodes to the dataframe\n",
    "for i in response.nodes:\n",
    "    if(\"addr:postcode\" in i.tags and \"name\" in i.tags):\n",
    "        tdf = pd.DataFrame(i.tags,index=[i.id])\n",
    "        tdf[\"type\"] = 'node'\n",
    "        rdf = pd.concat([rdf,tdf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection to AWS PostgreSQL and inserting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "        host=\"ads-database1.cbwqb3cep5ch.eu-central-1.rds.amazonaws.com\",\n",
    "        database=\"adsdatabase\",\n",
    "        user=\"adschief1\",\n",
    "        password=\"1eYvmDnMPikKSImeLrev\"\n",
    "    )\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table for ImmoScout24 in the database\n",
    "cur.execute(\n",
    "    '''CREATE TABLE IF NOT EXISTS immos (\n",
    "   \"id\" int PRIMARY KEY,\n",
    "   \"rooms\" FLOAT NOT NULL,\n",
    "   \"size\" INT NOT NULL,\n",
    "   \"price\" INT NOT NULL,\n",
    "   \"address\" VARCHAR ( 255 ),\n",
    "   \"zipcode\" INT NOT NULL,\n",
    "   \"city\"  VARCHAR ( 15 ) NOT NULL,\n",
    "   \"kanton\" VARCHAR ( 2 ) NOT NULL\n",
    ");'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\n",
    "    '''CREATE TABLE IF NOT EXISTS supermarkets (\n",
    "   \"id\" BIGINT PRIMARY KEY,\n",
    "   \"type\" VARCHAR ( 10 ) NOT NULL,\n",
    "   \"name\" VARCHAR ( 255 ),\n",
    "   \"zipcode\" INT NOT NULL,\n",
    "   \"city\" VARCHAR ( 50 ) ,\n",
    "   \"address\" VARCHAR ( 255 )\n",
    ");'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the data from ImmoScout24 into the database\n",
    "\n",
    "# Iterate over the properties array\n",
    "for i, rows in df.iterrows():\n",
    "    \n",
    "    # Extract the individual details\n",
    "    id = rows[\"ID\"]\n",
    "    rooms = rows[0]\n",
    "    size = rows[1]\n",
    "    price = rows[2]\n",
    "    address = rows[3]\n",
    "    zipcode = rows[4]\n",
    "    city = rows[5]\n",
    "    kanton = rows[6].lstrip()\n",
    "    \n",
    "    # Process the individual data record as needed\n",
    "    print(\"Rooms:\", rooms)\n",
    "    print(\"Size:\", size)\n",
    "    print(\"Price:\", price)\n",
    "    print(\"Address:\", address)\n",
    "    print(\"Zip-Code\", zipcode)\n",
    "    print(\"city\", city)\n",
    "    print(\"Kanton\", kanton)\n",
    "    print(\"--------------------\")\n",
    "\n",
    "\n",
    "    # Insert the data from ImmoScout24 into the database\n",
    "    cur.execute(\n",
    "        \"INSERT INTO immos (id, rooms, size, price, address, zipcode, city, kanton) VALUES (%s, %s, %s, %s, %s, %s, %s, %s) on conflict do nothing\",\n",
    "        (id,rooms, size, price, address, zipcode, city, kanton)\n",
    "    )\n",
    "\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "#cur.close()\n",
    "#conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, rows in rdf.iterrows():\n",
    "    \n",
    "    # Extract the individual details\n",
    "    id = i\n",
    "    type = rows[\"type\"]\n",
    "    name = rows[\"name\"]\n",
    "    address = str(rows[\"addr:street\"])+' '+str(rows[\"addr:housenumber\"])\n",
    "    zipcode = rows[\"addr:postcode\"]\n",
    "    city = rows[\"addr:city\"]\n",
    "\n",
    "\n",
    "    # Process the individual data record as needed\n",
    "    print(\"id:\", id)\n",
    "    print(\"type:\", type)\n",
    "    print(\"name:\", name)\n",
    "    print(\"Address:\", address)\n",
    "    print(\"Zip-Code\", zipcode)\n",
    "    print(\"city\", city)\n",
    "    print(\"--------------------\")\n",
    "\n",
    "    # Insert the data from Overpass into the database\n",
    "    cur.execute(\n",
    "        \"INSERT INTO supermarkets (id, type, name, address, zipcode, city) VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT DO NOTHING\",\n",
    "        (id, type, name, address, zipcode, city)\n",
    "    )\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "#cur.close()\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n",
    "cur.execute('''      \n",
    "UPDATE immos\n",
    "SET city = 'St.Gallen'\n",
    "WHERE city = 'St.';\n",
    "''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the two Tables \"immoscout24_data\" and \"overpass_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM immos\"\n",
    "immo_df = pd.read_sql(query, conn)\n",
    "immo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM supermarkets\"\n",
    "sm_df = pd.read_sql(query, conn)\n",
    "sm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get amount of markets per zip code\n",
    "dupli = sm_df.pivot_table(index = ['zipcode'], aggfunc ='size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupli.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge supermarket list with final dataset\n",
    "final = immo_df.merge(dupli.rename('supermarkets'),on='zipcode',how='left')\n",
    "final = final.fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA using our data from PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data types\n",
    "final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of missing values\n",
    "final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data from the database\n",
    "cur.execute(\"SELECT price FROM immos\")\n",
    "\n",
    "# Fetch all the rows of the query result\n",
    "rows = cur.fetchall()\n",
    "\n",
    "# Extract the values from the rows\n",
    "prices = [row[0] for row in rows]\n",
    "\n",
    "# Calculate the mean\n",
    "mean = statistics.mean(prices)\n",
    "\n",
    "# Calculate the median\n",
    "median = statistics.median(prices)\n",
    "\n",
    "# Calculate the mode\n",
    "mode = statistics.mode(prices)\n",
    "\n",
    "# Calculate the variance\n",
    "variance = statistics.variance(prices)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std_dev = statistics.stdev(prices)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Mode:\", mode)\n",
    "print(\"Variance:\", variance)\n",
    "print(\"Standard Deviation:\", std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of apartment prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(final['price'], bins=20, edgecolor='black')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Apartment Prices')\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of apartment price vs. square footage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(final['size'], final['price'])\n",
    "plt.xlabel('Square Footage')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Apartment Price vs. Square Footage')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we separate the data into training and testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (80% training, 20% testing)\n",
    "train_df, test_df = train_test_split(final, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create the correlation matrix \n",
    "correlation_matrix = train_df.corr()\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Plot (corrplot)\n",
    "sns.pairplot(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalization standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us standardize training set, creating the so-called Z-scores.\n",
    "train_df_mean = train_df.mean()\n",
    "train_df_std = train_df.std()\n",
    "train_df_stand = (train_df - train_df_mean)/train_df_std\n",
    "\n",
    "# Let us do the same for the test dataset\n",
    "test_df_mean = test_df.mean()\n",
    "test_df_std = test_df.std()\n",
    "test_df_stand = (test_df - test_df_mean)/test_df_std\n",
    "\n",
    "print(\"The data points have been standardized.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list that will eventually hold all created feature columns.\n",
    "feature_columns = []\n",
    "\n",
    "resolution_in_Zs = 0.3  # 3/10 of a standard deviation.\n",
    "\n",
    "\n",
    "# Create a bucket feature column for latitude.\n",
    "rooms_as_a_numeric_column = tf.feature_column.numeric_column(\"rooms\")\n",
    "rooms_boundaries = list(np.arange(int(min(train_df_stand['rooms'])), \n",
    "                                     int(max(train_df_stand['rooms'])), \n",
    "                                     resolution_in_Zs))\n",
    "rooms = tf.feature_column.bucketized_column(rooms_as_a_numeric_column, rooms_boundaries)\n",
    "\n",
    "# Create a bucket feature column for longitude.\n",
    "size_as_a_numeric_column = tf.feature_column.numeric_column(\"size\")\n",
    "size_boundaries = list(np.arange(int(min(train_df_stand['size'])), \n",
    "                                      int(max(train_df_stand['size'])), \n",
    "                                      resolution_in_Zs))\n",
    "size = tf.feature_column.bucketized_column(size_as_a_numeric_column, size_boundaries)\n",
    "\n",
    "# Create a feature cross of latitude and longitude.\n",
    "rooms_x_size = tf.feature_column.crossed_column([rooms, size], hash_bucket_size=100)\n",
    "crossed_feature = tf.feature_column.indicator_column(rooms_x_size)\n",
    "feature_columns.append(crossed_feature)  \n",
    "\n",
    "# Convert the list of feature columns into a layer that will later be fed into the model. \n",
    "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_learning_rate, feature_layer):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(feature_layer)\n",
    "\n",
    "  # Add one linear layer to the model to yield a simple linear regressor.\n",
    "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
    "\n",
    "  # Construct the layers into a model that TensorFlow can execute.\n",
    "  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model           \n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, batch_size, target_name):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.asarray(value).astype(np.float32) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(target_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True)\n",
    "\n",
    "  # Get details that will be useful for plotting the loss curve.\n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  rmse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, rmse   \n",
    "\n",
    "print(\"The create_model and the train_model functions are set.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_loss_curve(epochs, mse):\n",
    "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, mse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([mse.min()*0.95, mse.max() * 1.05])\n",
    "  plt.show()  \n",
    "\n",
    "print(\"The plot_the_loss_curve function is correctly defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df_stand.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.01 \n",
    "epochs = 500 \n",
    "batch_size = 300 \n",
    "\n",
    "# Here we define the target.\n",
    "target_name = \"price\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, my_feature_layer)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, mse = train_model(my_model, train_df_stand, epochs, batch_size, target_name)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "test_features = {name:np.asarray(value).astype(np.float32) for name, value in test_df_stand.items()}\n",
    "test_label = np.array(test_features.pop(target_name)) # isolate the label\n",
    "print(\"\\n Evaluate the linear regression model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_learning_rate, my_feature_layer):\n",
    "  \"\"\"Create and compile a simple network.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the layer containing the feature columns to the model.\n",
    "  model.add(my_feature_layer)\n",
    "\n",
    "  # Define the first hidden layer with 20 nodes.   \n",
    "  model.add(tf.keras.layers.Dense(units=20, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden1'))\n",
    "  \n",
    "  # Define the second hidden layer with 10 nodes (i.e., 20/2). \n",
    "  model.add(tf.keras.layers.Dense(units=10, \n",
    "                                  activation='relu', \n",
    "                                  name='Hidden2'))\n",
    "  \n",
    "  # Define the output layer.\n",
    "  model.add(tf.keras.layers.Dense(units=1,  \n",
    "                                  name='Output'))                              \n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, epochs, target_name,\n",
    "                batch_size=None):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Split the dataset into features and label.\n",
    "  features = {name:np.asarray(value).astype(np.float32) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(target_name))\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True) \n",
    "\n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # To track the progression of training, gather a snapshot of the model's mean squared error at each epoch. \n",
    "  hist = pd.DataFrame(history.history)\n",
    "  mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "  return epochs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "batch_size = 300\n",
    "\n",
    "# Specify the label\n",
    "target_name = \"price\"\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, my_feature_layer)\n",
    "\n",
    "# defined by the feature_layer.\n",
    "epochs, mse = train_model(my_model, train_df_stand, epochs, \n",
    "                          target_name, batch_size)\n",
    "plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "# After building a model against the training set, test that model\n",
    "# against the test set.\n",
    "test_features = {name:np.asarray(value).astype(np.float32) for name, value in test_df_stand.items()}\n",
    "test_target = np.array(test_features.pop(target_name)) # isolate the target\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x = test_features, y = test_target, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the convergence of both models, we can compare the test set loss for each. In our experiments, the loss of the linear regression was lower than that of the deep neural network (even if not dramatically lower), which suggests that the linear regression model will make better predictions than the deep neural network model.\n",
    "\n",
    "However, we need to take into consideration model complexity, as well as other issues like explainability and the fact that the loss and MSE on both models are high due to possibly lack of data or due to the quality of the data. \n",
    "\n",
    "If the market would be regulated, the linear regression would be preferred instead of the deep neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
